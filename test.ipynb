{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36deb672-3694-4b39-8592-845e143fcb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label from text: Brown Spot\n",
      "Predicted label from image: Brown Spot\n",
      "Predicted label from audio: Brown Spot\n",
      "Predicted label from all modalities: Brown Spot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BartModel, BartTokenizer\n",
    "import torchvision.models as models\n",
    "import torchaudio\n",
    "from PIL import Image\n",
    "\n",
    "#############################################\n",
    "# 1. Define the Audio Model (DeepSpeech2)   #\n",
    "#############################################\n",
    "class DeepSpeech2AudioModel(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_mels=128, conv_out_channels=32, \n",
    "                 rnn_hidden_size=256, num_rnn_layers=3, bidirectional=True, output_dim=128):\n",
    "        \"\"\"\n",
    "        A DeepSpeech2-inspired model:\n",
    "         - Computes a mel-spectrogram,\n",
    "         - Processes it through two convolutional layers,\n",
    "         - Feeds the output to a multi-layer bidirectional GRU,\n",
    "         - Pools over time and projects to a fixed-dimension embedding.\n",
    "        \"\"\"\n",
    "        super(DeepSpeech2AudioModel, self).__init__()\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, conv_out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_out_channels, conv_out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # After two conv layers, the frequency dimension is roughly n_mels // 4.\n",
    "        self.rnn_input_size = conv_out_channels * (n_mels // 4)\n",
    "        self.rnn = nn.GRU(input_size=self.rnn_input_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          num_layers=num_rnn_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional)\n",
    "        rnn_output_dim = rnn_hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        # waveform: [batch, T]\n",
    "        mel = self.melspec(waveform)         # [batch, n_mels, time]\n",
    "        mel = mel.unsqueeze(1)               # [batch, 1, n_mels, time]\n",
    "        conv_out = self.conv(mel)            # [batch, conv_out_channels, new_n_mels, new_time]\n",
    "        batch_size, channels, freq, time = conv_out.size()\n",
    "        conv_out = conv_out.permute(0, 3, 1, 2)  # [batch, new_time, channels, freq]\n",
    "        conv_out = conv_out.contiguous().view(batch_size, time, -1)  # [batch, new_time, channels*freq]\n",
    "        rnn_out, _ = self.rnn(conv_out)        # [batch, new_time, rnn_output_dim]\n",
    "        pooled = rnn_out.mean(dim=1)           # [batch, rnn_output_dim]\n",
    "        out = self.fc(pooled)                  # [batch, output_dim]\n",
    "        return out\n",
    "\n",
    "#############################################\n",
    "# 2. Define the Multimodal Classifier       #\n",
    "#############################################\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, text_model, image_model, audio_model,\n",
    "                 text_feat_dim, image_feat_dim, audio_feat_dim,\n",
    "                 hidden_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Combines text, image, and audio encoders. Each branch is projected\n",
    "        into a common hidden space and the features are averaged (if more than one\n",
    "        modality is provided) before classification.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "        self.audio_model = audio_model\n",
    "        \n",
    "        self.text_fc = nn.Linear(text_feat_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_feat_dim, hidden_dim)\n",
    "        self.audio_fc = nn.Linear(audio_feat_dim, hidden_dim)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, text_input=None, image_input=None, audio_input=None):\n",
    "        features = None\n",
    "        modality_count = 0\n",
    "        \n",
    "        if text_input is not None:\n",
    "            # Remove extra key \"labels\" if present\n",
    "            text_input_filtered = {k: v for k, v in text_input.items() if k != \"labels\"}\n",
    "            text_outputs = self.text_model(**text_input_filtered)\n",
    "            pooled_text = text_outputs.last_hidden_state.mean(dim=1)\n",
    "            text_features = self.text_fc(pooled_text)\n",
    "            features = text_features if features is None else features + text_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if image_input is not None:\n",
    "            image_features = self.image_model(image_input)\n",
    "            image_features = self.image_fc(image_features)\n",
    "            features = image_features if features is None else features + image_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if audio_input is not None:\n",
    "            audio_features = self.audio_model(audio_input)\n",
    "            audio_features = self.audio_fc(audio_features)\n",
    "            features = audio_features if features is None else features + audio_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if modality_count > 1:\n",
    "            features = features / modality_count\n",
    "            \n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "#############################################\n",
    "# 3. Define Inference Functions             #\n",
    "#############################################\n",
    "def inference_text(model, tokenizer, text, device, max_length=128):\n",
    "    \"\"\"Run inference on text input only.\"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=None, audio_input=None)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_image(model, image_path, transform, device):\n",
    "    \"\"\"Run inference on image input only.\"\"\"\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=None, image_input=image, audio_input=None)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_audio(model, audio_path, device, sample_rate=16000):\n",
    "    \"\"\"Run inference on audio input only.\"\"\"\n",
    "    model.eval()\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=None, image_input=None, audio_input=waveform)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_all(model, tokenizer, text, image_path, audio_path, transform, device, sample_rate=16000, max_length=128):\n",
    "    \"\"\"Run inference using all three modalities.\"\"\"\n",
    "    model.eval()\n",
    "    # Process text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    # Process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    # Process audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=image, audio_input=waveform)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "#############################################\n",
    "# 4. Set Up the Model and Load Weights      #\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model name and load the tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- Text Encoder (BART) ---\n",
    "text_encoder = BartModel.from_pretrained(model_name)\n",
    "text_encoder.to(device)\n",
    "text_feat_dim = text_encoder.config.d_model  # Typically ~768\n",
    "\n",
    "# --- Image Encoder (ResNet18) ---\n",
    "image_encoder = models.resnet18(pretrained=True)\n",
    "num_img_features = image_encoder.fc.in_features\n",
    "image_encoder.fc = nn.Identity()  # Remove the classification head\n",
    "image_encoder.to(device)\n",
    "image_feat_dim = num_img_features  # Typically ~512\n",
    "\n",
    "# --- Audio Encoder (DeepSpeech2-Inspired) ---\n",
    "audio_encoder = DeepSpeech2AudioModel(\n",
    "    sample_rate=16000, \n",
    "    n_mels=128, \n",
    "    conv_out_channels=32, \n",
    "    rnn_hidden_size=256, \n",
    "    num_rnn_layers=3, \n",
    "    bidirectional=True, \n",
    "    output_dim=128\n",
    ")\n",
    "audio_encoder.to(device)\n",
    "audio_feat_dim = 128\n",
    "\n",
    "# Define label list (update as needed) and create mapping\n",
    "label_list = [\n",
    "    \"Bacterial Leaf Blight\", \"Brown Spot\", \"Healthy\", \"Leaf Blast\", \n",
    "    \"Leaf Blight\", \"Leaf Scald\", \"Leaf Smut\", \"Narrow Brown Spot\"\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "num_classes = len(label_list)\n",
    "\n",
    "# --- Instantiate the Multimodal Classifier ---\n",
    "model = MultiModalClassifier(\n",
    "    text_model=text_encoder,\n",
    "    image_model=image_encoder,\n",
    "    audio_model=audio_encoder,\n",
    "    text_feat_dim=text_feat_dim,\n",
    "    image_feat_dim=image_feat_dim,\n",
    "    audio_feat_dim=audio_feat_dim,\n",
    "    hidden_dim=512,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Load the saved model weights (update the path if needed)\n",
    "MODEL_SAVE_PATH = \"multimodal_model.pth\"\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    raise FileNotFoundError(f\"Saved model not found at {MODEL_SAVE_PATH}\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#############################################\n",
    "# 5. Define Transforms for Image Inference   #\n",
    "#############################################\n",
    "image_inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#############################################\n",
    "# 6. Specify Sample Inputs and Run Inference  #\n",
    "#############################################\n",
    "# Update the file paths below as needed\n",
    "sample_text = (\"Small brown spots on leaves, spots may have yellow halos, lesions on leaf sheaths, spots may coalesce to form larger necrotic areas, leaf tip dieback, reduced grain quality\")\n",
    "sample_image_path = r\"F:\\ABDUL\\ABDUL 2024\\RICE PLANT DISEASE DETECTION YOLO\\FINAL SOURCE CODE\\MULITEMODEL_AI\\IMAGES\\train\\Brown Spot\\brown_spot (1).jpg\"\n",
    "sample_audio_path = r\"F:\\ABDUL\\ABDUL 2024\\RICE PLANT DISEASE DETECTION YOLO\\FINAL SOURCE CODE\\MULITEMODEL_AI\\AUDIO\\train\\Brown Spot\\Brown Spot_1.wav\"\n",
    "\n",
    "# Inference on each modality\n",
    "predicted_label_text = inference_text(model, tokenizer, sample_text, device)\n",
    "print(f\"Predicted label from text: {predicted_label_text}\")\n",
    "\n",
    "predicted_label_image = inference_image(model, sample_image_path, image_inference_transform, device)\n",
    "print(f\"Predicted label from image: {predicted_label_image}\")\n",
    "\n",
    "predicted_label_audio = inference_audio(model, sample_audio_path, device)\n",
    "print(f\"Predicted label from audio: {predicted_label_audio}\")\n",
    "\n",
    "# Inference using all modalities together\n",
    "predicted_label_all = inference_all(model, tokenizer, sample_text, sample_image_path, sample_audio_path,\n",
    "                                    image_inference_transform, device)\n",
    "print(f\"Predicted label from all modalities: {predicted_label_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e133f-e7a3-4ab0-b70e-1ebb2434b66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85924a4-2c18-4ede-a1a8-701a969468c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87286f92-941f-441c-9145-62c761f41413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label from text: Brown Spot\n",
      "Predicted label from image: Brown Spot\n",
      "Predicted label from audio: Brown Spot\n",
      "Predicted label from all modalities: Brown Spot\n",
      "\n",
      "Detected disease: Brown Spot\n",
      "\n",
      "Suggestion Report:\n",
      "Sure, here's a detailed report on managing the plant disease 'Brown Spot' in both English and Tamil:\n",
      "\n",
      "---\n",
      "\n",
      "### What it is: என்னுடையது\n",
      "**English:**\n",
      "Brown Spot is a fungal disease that affects various plants, causing brown, circular spots on leaves, stems, and sometimes fruits. These spots can merge and lead to significant damage, affecting the plant's overall health and yield.\n",
      "\n",
      "**Tamil:**\n",
      "பிரவுன் ஸ்பாட் ஒரு பூச்சி நோய் ஆகும், இது பல்வேறு தாவரங்களை பாதிப்பது, இதனால் இலை, கிளைகள், மற்றும் சில மாட்டுகளில் பிடிப்புகள் ஏற்படுகின்றன. இந்த பிடிப்புகள் இணைந்து முக்கிய சேதம் செய்யக்கூடியது, தாவரத்தின் மொத்த ஆரோக்கியத்தையும் உற்பத்தியையும் பாதிப்பது.\n",
      "\n",
      "### Why it occurs: எண்ணினால்\n",
      "**English:**\n",
      "Brown Spot occurs due to fungal infections, often caused by high humidity and poor air circulation. Nutrient-deficient soils and excessive nitrogen can also contribute to the disease. The fungus thrives in wet conditions and can spread through infected seeds, plant debris, and weeds.\n",
      "\n",
      "**Tamil:**\n",
      "பிரவுன் ஸ்பாட் பூச்சிகளினால் ஏற்படுகிறது, இது அதிக ஈரம் மற்றும் காற்றோட்டம் பாதிப்பதால் முக்கியமாக ஏற்படுகிறது. உணவு பற்றாக்குந்துகள் மற்றும் மிகுந்த நைட்ரஜன் பிரவுன் ஸ்பாட் நோயை உயர்த்தல் செய்கிறது. பூச்சிகள் மழையினால் வளர்கின்றன, மற்றும் பிடிப்புகள், தாவர சோர்வுகள், மற்றும் புதப்புகள் மூலம் பாதிப்புகள் பரவலாம்.\n",
      "\n",
      "### How to overcome: எப்படி எதிர்த்து செய்யலாம்\n",
      "**English:**\n",
      "1. **Remove Infected Plants:** Remove and destroy infected plants to prevent the spread of the fungus.\n",
      "2. **Improve Air Circulation:** Ensure proper spacing between plants to improve air circulation.\n",
      "3. **Use Fungicides:** Apply appropriate fungicides to control the spread of the disease.\n",
      "4. **Rotate Crops:** Rotate crops to reduce the buildup of fungal spores in the soil.\n",
      "5. **Maintain Soil Health:** Keep the soil well-drained and avoid overwatering.\n",
      "\n",
      "**Tamil:**\n",
      "1. **பிடிப்புகளை அகற்றுக:** பிடிப்புகளை அகற்றி அழிக்கவும்.\n",
      "2. **காற்றோட்டத்தை அதிகமாக்கு:** தாவரங்களிடையில் சரியான இடைவெளி உள்ளதாக உறுதிப்படுத்து.\n",
      "3. **பூச்சிக்கொல்லி பயன்படுத்து:** நோயை கட்டுப்படுத்த சரியான பூச்சிக்கொல்லிகளை பயன்படுத்து.\n",
      "4. **வகைகளை மாற்று:** வகைகளை மாற்றுவதன் மூலம் நோயின் பரவலை குறைக்க.\n",
      "5. **மண்ணின் ஆரோக்கியத்தை பராமரிக்க:** மண்ணை நன்கு செரிக்கவும், மிகுந்த தண்ணீர் பயன்படுத்தாமல்.\n",
      "\n",
      "### Fertilizer Recommendations: உணவு பரிந்தைகள்\n",
      "**English:**\n",
      "Use a balanced fertilizer with a lower nitrogen content to avoid promoting fungal growth. Ensure the soil has adequate potassium and phosphorus levels to support plant health.\n",
      "\n",
      "**Tamil:**\n",
      "பிடிப்புகளை குறைக்க மற்றும் பூச்சிகளின் வளர்ச்சியை தவிர்க்க மாதிரி உணவு பரிந்தைகளை பயன்படுத்து. தாவரங்களின் ஆரோக்கியத்தை ஆதரிக்க மண்ணில் கார்பனேட் மற்றும் பொசுபரஸ் அளவுகள் பூச்சிகளின் வளர்ச்சியை தவிர்க்க.\n",
      "\n",
      "---\n",
      "\n",
      "I hope this helps! If you have any more questions or need further assistance, feel free to ask.\n",
      "Suggestion report saved to suggestion_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py:84: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from asyncio import WindowsSelectorEventLoopPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BartModel, BartTokenizer\n",
    "import torchvision.models as models\n",
    "import torchaudio\n",
    "from PIL import Image\n",
    "import g4f\n",
    "\n",
    "#############################################\n",
    "# 1. Define the Audio Model (DeepSpeech2)   #\n",
    "#############################################\n",
    "class DeepSpeech2AudioModel(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_mels=128, conv_out_channels=32, \n",
    "                 rnn_hidden_size=256, num_rnn_layers=3, bidirectional=True, output_dim=128):\n",
    "        \"\"\"\n",
    "        A DeepSpeech2-inspired model:\n",
    "         - Computes a mel-spectrogram,\n",
    "         - Processes it through two convolutional layers,\n",
    "         - Feeds the output to a multi-layer bidirectional GRU,\n",
    "         - Pools over time and projects to a fixed-dimension embedding.\n",
    "        \"\"\"\n",
    "        super(DeepSpeech2AudioModel, self).__init__()\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, conv_out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_out_channels, conv_out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # After two conv layers, the frequency dimension is roughly n_mels // 4.\n",
    "        self.rnn_input_size = conv_out_channels * (n_mels // 4)\n",
    "        self.rnn = nn.GRU(input_size=self.rnn_input_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          num_layers=num_rnn_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional)\n",
    "        rnn_output_dim = rnn_hidden_size * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(rnn_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        # waveform: [batch, T]\n",
    "        mel = self.melspec(waveform)         # [batch, n_mels, time]\n",
    "        mel = mel.unsqueeze(1)               # [batch, 1, n_mels, time]\n",
    "        conv_out = self.conv(mel)            # [batch, conv_out_channels, new_n_mels, new_time]\n",
    "        batch_size, channels, freq, time_steps = conv_out.size()\n",
    "        conv_out = conv_out.permute(0, 3, 1, 2)  # [batch, new_time, channels, freq]\n",
    "        conv_out = conv_out.contiguous().view(batch_size, time_steps, -1)  # [batch, new_time, channels*freq]\n",
    "        rnn_out, _ = self.rnn(conv_out)        # [batch, new_time, rnn_output_dim]\n",
    "        pooled = rnn_out.mean(dim=1)           # [batch, rnn_output_dim]\n",
    "        out = self.fc(pooled)                  # [batch, output_dim]\n",
    "        return out\n",
    "\n",
    "#############################################\n",
    "# 2. Define the Multimodal Classifier       #\n",
    "#############################################\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, text_model, image_model, audio_model,\n",
    "                 text_feat_dim, image_feat_dim, audio_feat_dim,\n",
    "                 hidden_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Combines text, image, and audio encoders. Each branch is projected\n",
    "        into a common hidden space and the features are averaged (if more than one\n",
    "        modality is provided) before classification.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "        self.audio_model = audio_model\n",
    "        \n",
    "        self.text_fc = nn.Linear(text_feat_dim, hidden_dim)\n",
    "        self.image_fc = nn.Linear(image_feat_dim, hidden_dim)\n",
    "        self.audio_fc = nn.Linear(audio_feat_dim, hidden_dim)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, text_input=None, image_input=None, audio_input=None):\n",
    "        features = None\n",
    "        modality_count = 0\n",
    "        \n",
    "        if text_input is not None:\n",
    "            # Remove extra key \"labels\" if present\n",
    "            text_input_filtered = {k: v for k, v in text_input.items() if k != \"labels\"}\n",
    "            text_outputs = self.text_model(**text_input_filtered)\n",
    "            pooled_text = text_outputs.last_hidden_state.mean(dim=1)\n",
    "            text_features = self.text_fc(pooled_text)\n",
    "            features = text_features if features is None else features + text_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if image_input is not None:\n",
    "            image_features = self.image_model(image_input)\n",
    "            image_features = self.image_fc(image_features)\n",
    "            features = image_features if features is None else features + image_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if audio_input is not None:\n",
    "            audio_features = self.audio_model(audio_input)\n",
    "            audio_features = self.audio_fc(audio_features)\n",
    "            features = audio_features if features is None else features + audio_features\n",
    "            modality_count += 1\n",
    "            \n",
    "        if modality_count > 1:\n",
    "            features = features / modality_count\n",
    "            \n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "#############################################\n",
    "# 3. Define Inference Functions             #\n",
    "#############################################\n",
    "def inference_text(model, tokenizer, text, device, max_length=128):\n",
    "    \"\"\"Run inference on text input only.\"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=None, audio_input=None)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_image(model, image_path, transform, device):\n",
    "    \"\"\"Run inference on image input only.\"\"\"\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=None, image_input=image, audio_input=None)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_audio(model, audio_path, device, sample_rate=16000):\n",
    "    \"\"\"Run inference on audio input only.\"\"\"\n",
    "    model.eval()\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=None, image_input=None, audio_input=waveform)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "def inference_all(model, tokenizer, text, image_path, audio_path, transform, device, sample_rate=16000, max_length=128):\n",
    "    \"\"\"Run inference using all three modalities.\"\"\"\n",
    "    model.eval()\n",
    "    # Process text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "    # Process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    # Process audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_input=encoding, image_input=image, audio_input=waveform)\n",
    "    pred_id = torch.argmax(logits, dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "#############################################\n",
    "# 4. Set Up the Model and Load Weights      #\n",
    "#############################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model name and load the tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- Text Encoder (BART) ---\n",
    "text_encoder = BartModel.from_pretrained(model_name)\n",
    "text_encoder.to(device)\n",
    "text_feat_dim = text_encoder.config.d_model  # Typically ~768\n",
    "\n",
    "# --- Image Encoder (ResNet18) ---\n",
    "image_encoder = models.resnet18(pretrained=True)\n",
    "num_img_features = image_encoder.fc.in_features\n",
    "image_encoder.fc = nn.Identity()  # Remove the classification head\n",
    "image_encoder.to(device)\n",
    "image_feat_dim = num_img_features  # Typically ~512\n",
    "\n",
    "# --- Audio Encoder (DeepSpeech2-Inspired) ---\n",
    "audio_encoder = DeepSpeech2AudioModel(\n",
    "    sample_rate=16000, \n",
    "    n_mels=128, \n",
    "    conv_out_channels=32, \n",
    "    rnn_hidden_size=256, \n",
    "    num_rnn_layers=3, \n",
    "    bidirectional=True, \n",
    "    output_dim=128\n",
    ")\n",
    "audio_encoder.to(device)\n",
    "audio_feat_dim = 128\n",
    "\n",
    "# Define label list (update as needed) and create mapping\n",
    "label_list = [\n",
    "    \"Bacterial Leaf Blight\", \"Brown Spot\", \"Healthy\", \"Leaf Blast\", \n",
    "    \"Leaf Blight\", \"Leaf Scald\", \"Leaf Smut\", \"Narrow Brown Spot\"\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "num_classes = len(label_list)\n",
    "\n",
    "# --- Instantiate the Multimodal Classifier ---\n",
    "model = MultiModalClassifier(\n",
    "    text_model=text_encoder,\n",
    "    image_model=image_encoder,\n",
    "    audio_model=audio_encoder,\n",
    "    text_feat_dim=text_feat_dim,\n",
    "    image_feat_dim=image_feat_dim,\n",
    "    audio_feat_dim=audio_feat_dim,\n",
    "    hidden_dim=512,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Load the saved model weights (update the path if needed)\n",
    "MODEL_SAVE_PATH = \"multimodal_model.pth\"\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    raise FileNotFoundError(f\"Saved model not found at {MODEL_SAVE_PATH}\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#############################################\n",
    "# 5. Define Transforms for Image Inference   #\n",
    "#############################################\n",
    "image_inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#############################################\n",
    "# 6. Specify Sample Inputs and Run Inference  #\n",
    "#############################################\n",
    "# Update the file paths below as needed\n",
    "sample_text = (\n",
    "    \"Small brown spots on leaves, spots may have yellow halos, lesions on leaf sheaths, \"\n",
    "    \"spots may coalesce to form larger necrotic areas, leaf tip dieback, reduced grain quality\"\n",
    ")\n",
    "sample_image_path = r\"F:\\ABDUL\\ABDUL 2024\\RICE PLANT DISEASE DETECTION YOLO\\FINAL SOURCE CODE\\MULITEMODEL_AI\\IMAGES\\train\\Brown Spot\\brown_spot (1).jpg\"\n",
    "sample_audio_path = r\"F:\\ABDUL\\ABDUL 2024\\RICE PLANT DISEASE DETECTION YOLO\\FINAL SOURCE CODE\\MULITEMODEL_AI\\AUDIO\\train\\Brown Spot\\Brown Spot_1.wav\"\n",
    "\n",
    "# Inference on individual modalities\n",
    "predicted_label_text = inference_text(model, tokenizer, sample_text, device)\n",
    "print(f\"Predicted label from text: {predicted_label_text}\")\n",
    "\n",
    "predicted_label_image = inference_image(model, sample_image_path, image_inference_transform, device)\n",
    "print(f\"Predicted label from image: {predicted_label_image}\")\n",
    "\n",
    "predicted_label_audio = inference_audio(model, sample_audio_path, device)\n",
    "print(f\"Predicted label from audio: {predicted_label_audio}\")\n",
    "\n",
    "# Inference using all modalities together\n",
    "predicted_label_all = inference_all(model, tokenizer, sample_text, sample_image_path, sample_audio_path,\n",
    "                                    image_inference_transform, device)\n",
    "print(f\"Predicted label from all modalities: {predicted_label_all}\")\n",
    "\n",
    "#############################################\n",
    "# 7. Generate Suggestion Report via GPT-4   #\n",
    "#############################################\n",
    "# Set the event loop policy for Windows\n",
    "asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "def generate_response(user_input):\n",
    "    \"\"\"Generate a response using GPT-4 via g4f.\"\"\"\n",
    "    try:\n",
    "        response = g4f.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        return response.strip() if response else \"No response generated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\"\n",
    "\n",
    "def generate_suggestion_report(disease_label):\n",
    "    \"\"\"\n",
    "    Generate a detailed, step-by-step suggestion report for the detected disease.\n",
    "    The report includes:\n",
    "      1. What it is\n",
    "      2. Why it occurs\n",
    "      3. How to overcome\n",
    "      4. Fertilizer Recommendations\n",
    "    The report is provided in both English and Tamil.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Provide a detailed, step-by-step suggestion report for the plant disease '{disease_label}' detected. \"\n",
    "        \"The report should have the following sections with clear headings:\\n\"\n",
    "        \"1. What it is: Explain what the disease is in simple terms.\\n\"\n",
    "        \"2. Why it occurs: Describe the causes and contributing factors for the disease.\\n\"\n",
    "        \"3. How to overcome: Provide a step-by-step guide on how to manage and overcome the disease.\\n\"\n",
    "        \"4. Fertilizer Recommendations: Suggest the type of fertilizer and application methods suitable for this condition.\\n\"\n",
    "        \"Please provide the report in both English and Tamil, with each section written in both languages.\"\n",
    "    )\n",
    "    return generate_response(prompt)\n",
    "\n",
    "# Generate suggestion report based on the detected disease\n",
    "detected_disease = predicted_label_all\n",
    "print(f\"\\nDetected disease: {detected_disease}\")\n",
    "suggestion_report = generate_suggestion_report(detected_disease)\n",
    "print(\"\\nSuggestion Report:\")\n",
    "print(suggestion_report)\n",
    "\n",
    "# Save the suggestion report to a text file (download the report)\n",
    "report_filename = \"suggestion_report.txt\"\n",
    "with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(suggestion_report)\n",
    "print(f\"Suggestion report saved to {report_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283052b-e9d3-49d7-8901-3028035c9ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
